{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # reads .env into environment\n",
    "\n",
    "# OpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"OPENAI_API_KEY\"]  # forces KeyError if missing\n",
    "\n",
    "# LangSmith (LangChain)\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.environ[\"LANGCHAIN_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\", \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_openai\n",
      "  Using cached langchain_openai-1.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.1.0 (from langchain_openai)\n",
      "  Using cached langchain_core-1.1.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting openai<3.0.0,>=1.109.1 (from langchain_openai)\n",
      "  Using cached openai-2.9.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting tiktoken<1.0.0,>=0.7.0 (from langchain_openai)\n",
      "  Downloading tiktoken-0.12.0-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Using cached langsmith-0.4.56-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting packaging<26.0.0,>=23.2.0 (from langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_openai) (2.10.3)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_openai) (6.0.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_openai) (8.2.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_openai) (4.12.2)\n",
      "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Using cached uuid_utils-0.12.0-cp39-abi3-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.26.0)\n",
      "Collecting jiter<1,>=0.10.0 (from openai<3.0.0,>=1.109.1->langchain_openai)\n",
      "  Downloading jiter-0.12.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.65.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain_openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain_openai) (2.1)\n",
      "Collecting orjson>=3.9.14 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Downloading orjson-3.11.5-cp311-cp311-win_amd64.whl.metadata (42 kB)\n",
      "     ---------------------------------------- 0.0/42.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/42.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/42.7 kB ? eta -:--:--\n",
      "     --------------------------- ---------- 30.7/42.7 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------------------------------- 42.7/42.7 kB 345.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai) (1.0.0)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Downloading zstandard-0.25.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain_openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain_openai) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.0.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=1.109.1->langchain_openai) (0.4.6)\n",
      "Using cached langchain_openai-1.1.0-py3-none-any.whl (84 kB)\n",
      "Using cached langchain_core-1.1.1-py3-none-any.whl (475 kB)\n",
      "Using cached openai-2.9.0-py3-none-any.whl (1.0 MB)\n",
      "Downloading tiktoken-0.12.0-cp311-cp311-win_amd64.whl (879 kB)\n",
      "   ---------------------------------------- 0.0/879.4 kB ? eta -:--:--\n",
      "   -------- ------------------------------ 194.6/879.4 kB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 879.4/879.4 kB 13.8 MB/s eta 0:00:00\n",
      "Downloading jiter-0.12.0-cp311-cp311-win_amd64.whl (204 kB)\n",
      "   ---------------------------------------- 0.0/204.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 204.9/204.9 kB 12.2 MB/s eta 0:00:00\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langsmith-0.4.56-py3-none-any.whl (411 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached uuid_utils-0.12.0-cp39-abi3-win_amd64.whl (183 kB)\n",
      "Downloading orjson-3.11.5-cp311-cp311-win_amd64.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.2/133.2 kB 7.7 MB/s eta 0:00:00\n",
      "Downloading zstandard-0.25.0-cp311-cp311-win_amd64.whl (506 kB)\n",
      "   ---------------------------------------- 0.0/506.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 506.2/506.2 kB 31.0 MB/s eta 0:00:00\n",
      "Installing collected packages: zstandard, uuid-utils, packaging, orjson, jsonpatch, jiter, tiktoken, openai, langsmith, langchain-core, langchain_openai\n",
      "  Attempting uninstall: zstandard\n",
      "    Found existing installation: zstandard 0.19.0\n",
      "    Uninstalling zstandard-0.19.0:\n",
      "      Successfully uninstalled zstandard-0.19.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.32\n",
      "    Uninstalling jsonpatch-1.32:\n",
      "      Successfully uninstalled jsonpatch-1.32\n",
      "Successfully installed jiter-0.12.0 jsonpatch-1.33 langchain-core-1.1.1 langchain_openai-1.1.0 langsmith-0.4.56 openai-2.9.0 orjson-3.11.5 packaging-25.0 tiktoken-0.12.0 uuid-utils-0.12.0 zstandard-0.25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-profiling 3.2.0 requires visions[type_image_path]==0.7.4, but you have visions 0.8.1 which is incompatible.\n",
      "streamlit 1.30.0 requires packaging<24,>=16.8, but you have packaging 25.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile={'max_input_tokens': 200000, 'max_output_tokens': 100000, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True} client=<openai.resources.chat.completions.completions.Completions object at 0x0000017C35C4D290> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000017C35CB4190> root_client=<openai.OpenAI object at 0x0000017C31E24750> root_async_client=<openai.AsyncOpenAI object at 0x0000017C35C74790> model_name='o4-mini' model_kwargs={} openai_api_key=SecretStr('**********') stream_usage=True\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"o4-mini\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babbage-002\n",
      "chatgpt-4o-latest\n",
      "codex-mini-latest\n",
      "dall-e-2\n",
      "dall-e-3\n",
      "davinci-002\n",
      "gpt-3.5-turbo\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-3.5-turbo-16k\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "gpt-4\n",
      "gpt-4-0125-preview\n",
      "gpt-4-0613\n",
      "gpt-4-1106-preview\n",
      "gpt-4-turbo\n",
      "gpt-4-turbo-2024-04-09\n",
      "gpt-4-turbo-preview\n",
      "gpt-4.1\n",
      "gpt-4.1-2025-04-14\n",
      "gpt-4.1-mini\n",
      "gpt-4.1-mini-2025-04-14\n",
      "gpt-4.1-nano\n",
      "gpt-4.1-nano-2025-04-14\n",
      "gpt-4o\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-2024-08-06\n",
      "gpt-4o-2024-11-20\n",
      "gpt-4o-audio-preview\n",
      "gpt-4o-audio-preview-2024-12-17\n",
      "gpt-4o-audio-preview-2025-06-03\n",
      "gpt-4o-mini\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4o-mini-audio-preview\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "gpt-4o-mini-realtime-preview\n",
      "gpt-4o-mini-realtime-preview-2024-12-17\n",
      "gpt-4o-mini-search-preview\n",
      "gpt-4o-mini-search-preview-2025-03-11\n",
      "gpt-4o-mini-transcribe\n",
      "gpt-4o-mini-tts\n",
      "gpt-4o-realtime-preview\n",
      "gpt-4o-realtime-preview-2024-12-17\n",
      "gpt-4o-realtime-preview-2025-06-03\n",
      "gpt-4o-search-preview\n",
      "gpt-4o-search-preview-2025-03-11\n",
      "gpt-4o-transcribe\n",
      "gpt-4o-transcribe-diarize\n",
      "gpt-5\n",
      "gpt-5-2025-08-07\n",
      "gpt-5-chat-latest\n",
      "gpt-5-codex\n",
      "gpt-5-mini\n",
      "gpt-5-mini-2025-08-07\n",
      "gpt-5-nano\n",
      "gpt-5-nano-2025-08-07\n",
      "gpt-5-pro\n",
      "gpt-5-pro-2025-10-06\n",
      "gpt-5-search-api\n",
      "gpt-5-search-api-2025-10-14\n",
      "gpt-5.1\n",
      "gpt-5.1-2025-11-13\n",
      "gpt-5.1-chat-latest\n",
      "gpt-5.1-codex\n",
      "gpt-5.1-codex-max\n",
      "gpt-5.1-codex-mini\n",
      "gpt-audio\n",
      "gpt-audio-2025-08-28\n",
      "gpt-audio-mini\n",
      "gpt-audio-mini-2025-10-06\n",
      "gpt-image-1\n",
      "gpt-image-1-mini\n",
      "gpt-realtime\n",
      "gpt-realtime-2025-08-28\n",
      "gpt-realtime-mini\n",
      "gpt-realtime-mini-2025-10-06\n",
      "o1\n",
      "o1-2024-12-17\n",
      "o1-pro\n",
      "o1-pro-2025-03-19\n",
      "o3\n",
      "o3-2025-04-16\n",
      "o3-mini\n",
      "o3-mini-2025-01-31\n",
      "o4-mini\n",
      "o4-mini-2025-04-16\n",
      "o4-mini-deep-research\n",
      "o4-mini-deep-research-2025-06-26\n",
      "omni-moderation-2024-09-26\n",
      "omni-moderation-latest\n",
      "sora-2\n",
      "sora-2-pro\n",
      "text-embedding-3-large\n",
      "text-embedding-3-small\n",
      "text-embedding-ada-002\n",
      "tts-1\n",
      "tts-1-1106\n",
      "tts-1-hd\n",
      "tts-1-hd-1106\n",
      "whisper-1\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "        \n",
    "client = OpenAI()\n",
    "models = client.models.list()\n",
    "\n",
    "# print model ids you can actually use with your API key\n",
    "for m in sorted(models.data, key=lambda x: x.id):\n",
    "    print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Agentic AI (sometimes called “autonomous agents” or “AI agents”) refers to artificial‐intelligence systems that not only generate predictions or responses but also take autonomous, goal‐driven actions in an environment. In other words, an agentic AI perceives its world, plans steps toward an objective, executes those steps, monitors outcomes, and adapts as needed—without requiring a human in the loop for every decision.  \\n\\nKey characteristics of agentic AI  \\n1. Goal orientation  \\n   • The agent has one or more objectives (for example, book a flight, optimize a supply chain, win a game).  \\n2. Perception and state–updating  \\n   • It takes inputs—observations, sensor data, API responses, user queries—and incorporates them into an internal model of its current state.  \\n3. Planning and decision making  \\n   • Using that model, it selects actions predicted to bring it closer to its goal. Techniques range from simple rule‐based decision trees to complex reinforcement‐learning policies or search/planning algorithms.  \\n4. Action execution  \\n   • It issues commands or API calls, manipulates software or physical devices, or interacts with humans to carry out chosen actions.  \\n5. Feedback and adaptation  \\n   • After observing the results of its actions, the agent updates its model or policy to learn from success and failure.  \\n\\nExamples of agentic AI  \\n• Reinforcement‐learning agents in games: e.g. DeepMind’s AlphaZero learns to play chess or Go by self‐play, planning its next move to maximize chances of winning.  \\n• Autonomous vehicles: perceive the road, decide steering, acceleration, braking in real time, and adapt to changing traffic conditions.  \\n• E-commerce “shopping assistants”: autonomously browse product catalogs, compare prices, place orders, and handle post‐purchase tasks.  \\n• AI research toolchains (AutoML, AutoGPT variants): chain together model building, evaluation, and deployment steps without human intervention.  \\n\\nAgentic vs. non-agentic AI  \\n• Non-agentic (reactive) systems generate outputs in response to inputs but do not set their own objectives or take multi‐step actions. e.g. a spam‐filter model or a stateless chatbot that answers each question independently.  \\n• Agentic systems carry state, hold plans, and can initiate new actions toward overarching goals.  \\n\\nBenefits and challenges  \\n+ Increased automation and efficiency: agents can operate 24/7, juggling many tasks at once.  \\n+ Greater autonomy: reduce human burden, accelerate decision cycles.  \\n– Safety and alignment risks: if objectives are misspecified or incentives misaligned, agents may pursue harmful shortcuts.  \\n– Transparency and control: understanding why an agent made a sequence of decisions can be complex.  \\n\\nIn short, agentic AI represents the shift from “smart tools” that require step‐by‐step human guidance toward “virtual actors” that can autonomously pursue goals—something that raises both exciting possibilities and important questions about how we design, oversee, and constrain such systems.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 766, 'prompt_tokens': 11, 'total_tokens': 777, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'o4-mini-2025-04-16', 'system_fingerprint': None, 'id': 'chatcmpl-Ck0jz44qJYPO3ZI8baS2LGtb6JNYK', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019af715-783b-7e11-8320-3aab86623a89-0' usage_metadata={'input_tokens': 11, 'output_tokens': 766, 'total_tokens': 777, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}\n"
     ]
    }
   ],
   "source": [
    "result=llm.invoke(\"What is agentic AI\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agentic AI (sometimes called “autonomous agents” or “AI agents”) refers to artificial‐intelligence systems that not only generate predictions or responses but also take autonomous, goal‐driven actions in an environment. In other words, an agentic AI perceives its world, plans steps toward an objective, executes those steps, monitors outcomes, and adapts as needed—without requiring a human in the loop for every decision.  \n",
      "\n",
      "Key characteristics of agentic AI  \n",
      "1. Goal orientation  \n",
      "   • The agent has one or more objectives (for example, book a flight, optimize a supply chain, win a game).  \n",
      "2. Perception and state–updating  \n",
      "   • It takes inputs—observations, sensor data, API responses, user queries—and incorporates them into an internal model of its current state.  \n",
      "3. Planning and decision making  \n",
      "   • Using that model, it selects actions predicted to bring it closer to its goal. Techniques range from simple rule‐based decision trees to complex reinforcement‐learning policies or search/planning algorithms.  \n",
      "4. Action execution  \n",
      "   • It issues commands or API calls, manipulates software or physical devices, or interacts with humans to carry out chosen actions.  \n",
      "5. Feedback and adaptation  \n",
      "   • After observing the results of its actions, the agent updates its model or policy to learn from success and failure.  \n",
      "\n",
      "Examples of agentic AI  \n",
      "• Reinforcement‐learning agents in games: e.g. DeepMind’s AlphaZero learns to play chess or Go by self‐play, planning its next move to maximize chances of winning.  \n",
      "• Autonomous vehicles: perceive the road, decide steering, acceleration, braking in real time, and adapt to changing traffic conditions.  \n",
      "• E-commerce “shopping assistants”: autonomously browse product catalogs, compare prices, place orders, and handle post‐purchase tasks.  \n",
      "• AI research toolchains (AutoML, AutoGPT variants): chain together model building, evaluation, and deployment steps without human intervention.  \n",
      "\n",
      "Agentic vs. non-agentic AI  \n",
      "• Non-agentic (reactive) systems generate outputs in response to inputs but do not set their own objectives or take multi‐step actions. e.g. a spam‐filter model or a stateless chatbot that answers each question independently.  \n",
      "• Agentic systems carry state, hold plans, and can initiate new actions toward overarching goals.  \n",
      "\n",
      "Benefits and challenges  \n",
      "+ Increased automation and efficiency: agents can operate 24/7, juggling many tasks at once.  \n",
      "+ Greater autonomy: reduce human burden, accelerate decision cycles.  \n",
      "– Safety and alignment risks: if objectives are misspecified or incentives misaligned, agents may pursue harmful shortcuts.  \n",
      "– Transparency and control: understanding why an agent made a sequence of decisions can be complex.  \n",
      "\n",
      "In short, agentic AI represents the shift from “smart tools” that require step‐by‐step human guidance toward “virtual actors” that can autonomously pursue goals—something that raises both exciting possibilities and important questions about how we design, oversee, and constrain such systems.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "\n",
    "    ]\n",
    ")\n",
    "prompt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Langsmith is a suite of tools developed by LangChain aimed at improving the development, debugging, and monitoring of applications that utilize large language models (LLMs). It provides a comprehensive platform for managing and evaluating the performance of language models in various applications.\\n\\nKey features of Langsmith include:\\n\\n1. **Experimentation**: It allows developers to run experiments to test different prompts, models, or configurations quickly and efficiently.\\n\\n2. **Comparison**: Langsmith provides functionalities to compare outputs from different LLMs under various conditions, helping in selecting the best model and prompt for a particular task.\\n\\n3. **Monitoring**: It offers monitoring tools that provide insights into the performance and usage of language models in production, assisting in maintaining robustness and reliability.\\n\\n4. **Debugging**: Langsmith aids in identifying issues and refining models and prompts, helping to enhance the accuracy and relevance of responses generated by LLMs.\\n\\n5. **Scalability**: The tools are designed to support scalability, making it easier for developers to adapt their applications to handle growing amounts of data and user requests.\\n\\nBy integrating these capabilities, Langsmith acts as a vital platform for developers looking to leverage language models effectively, enabling them to manage end-to-end workflows with more efficiency and confidence.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 253, 'prompt_tokens': 32, 'total_tokens': 285, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_37d212baff', 'id': 'chatcmpl-Ck0kePo2G0VJtb1ERAOxYzQkcVxsb', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019af716-15dc-7c21-b771-8c5ad135d2a7-0' usage_metadata={'input_tokens': 32, 'output_tokens': 253, 'total_tokens': 285, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|llm \n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is a suite of tools developed by LangChain aimed at improving the development, debugging, and monitoring of applications that utilize large language models (LLMs). It provides a comprehensive platform for managing and evaluating the performance of language models in various applications.\n",
      "\n",
      "Key features of Langsmith include:\n",
      "\n",
      "1. **Experimentation**: It allows developers to run experiments to test different prompts, models, or configurations quickly and efficiently.\n",
      "\n",
      "2. **Comparison**: Langsmith provides functionalities to compare outputs from different LLMs under various conditions, helping in selecting the best model and prompt for a particular task.\n",
      "\n",
      "3. **Monitoring**: It offers monitoring tools that provide insights into the performance and usage of language models in production, assisting in maintaining robustness and reliability.\n",
      "\n",
      "4. **Debugging**: Langsmith aids in identifying issues and refining models and prompts, helping to enhance the accuracy and relevance of responses generated by LLMs.\n",
      "\n",
      "5. **Scalability**: The tools are designed to support scalability, making it easier for developers to adapt their applications to handle growing amounts of data and user requests.\n",
      "\n",
      "By integrating these capabilities, Langsmith acts as a vital platform for developers looking to leverage language models effectively, enabling them to manage end-to-end workflows with more efficiency and confidence.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is a suite of tools designed to enhance the development, debugging, and monitoring of applications that are built using Language Models, particularly with LangChain, an open-source framework for building applications using large language models (LLMs). Langsmith supports both JavaScript and Python and is aimed at optimizing the quality and performance of LLM-driven applications.\n",
      "\n",
      "With Langsmith, developers have access to a range of features that facilitate better management of language models. It offers capabilities for real-time debugging, which helps developers identify and resolve issues early in the development process. Moreover, Langsmith provides monitoring tools that enable developers to track the performance of their applications, ensuring that they function optimally and deliver accurate results. By focusing on quality and performance, Langsmith helps developers build reliable and efficient applications with language models.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is a platform designed for the development, testing, and monitoring of language model applications and agents. It provides tools and services to facilitate the creation of advanced language processing applications, focusing on seamless integration and efficient management.', 'features': ['Development Tools: Offer utilities and libraries to streamline the development of language model applications.', 'Testing Environment: Provides a controlled space for rigorously testing language models under various scenarios.', 'Monitoring Capabilities: Allows for real-time tracking and analysis of language model performance to ensure optimal operation.', 'Integration: Supports integration with various language models and APIs to extend functionality and improve accessibility.', 'User-Friendly Interface: Designed to be intuitive, making it easier for developers to manage their projects.'], 'target_audience': ['Developers working on language model applications', 'Businesses looking to leverage language models for customer service or content creation', 'Researchers in the fields of NLP and AI']}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instructions': 'Return a JSON object.'}, template='Answer the user query.\\n{format_instructions}\\n{query}\\n')\n",
       "| ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x0000017C35E45350>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000017C33D14090>, root_client=<openai.OpenAI object at 0x0000017C35E6B950>, root_async_client=<openai.AsyncOpenAI object at 0x0000017C31A17250>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| JsonOutputParser()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_community\n",
      "  Using cached langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain_community) (1.1.1)\n",
      "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain_community)\n",
      "  Using cached langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain_community) (2.0.25)\n",
      "Collecting requests<3.0.0,>=2.32.5 (from langchain_community)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain_community) (3.9.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain_community) (8.2.2)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain_community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain_community)\n",
      "  Using cached pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain_community) (0.4.56)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
      "  Using cached httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain_community)\n",
      "  Using cached langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.10.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (25.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (0.12.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.26.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.25.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (0.21.0)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain_community)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2025.4.26)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain_community) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.27.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.0.0)\n",
      "Using cached langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Using cached langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
      "Using cached pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, typing-inspect, requests, marshmallow, httpx-sse, dataclasses-json, pydantic-settings, langchain-text-splitters, langchain-classic, langchain_community\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "  Attempting uninstall: pydantic-settings\n",
      "    Found existing installation: pydantic-settings 2.6.1\n",
      "    Uninstalling pydantic-settings-2.6.1:\n",
      "      Successfully uninstalled pydantic-settings-2.6.1\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.3 langchain-classic-1.0.0 langchain-text-splitters-1.0.0 langchain_community-0.4.1 marshmallow-3.26.1 pydantic-settings-2.12.0 requests-2.32.5 typing-inspect-0.9.0 typing-inspection-0.4.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aext-assistant-server 4.0.15 requires anaconda-cloud-auth, which is not installed.\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires requests==2.31.0, but you have requests 2.32.5 which is incompatible.\n",
      "pandas-profiling 3.2.0 requires visions[type_image_path]==0.7.4, but you have visions 0.8.1 which is incompatible.\n",
      "streamlit 1.30.0 requires packaging<24,>=16.8, but you have packaging 25.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x17c39bcdf50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://python.langchain.com/docs/tutorials/llm_chain/\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentğŸš€ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentSupervisor agentVoice agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy pageâ€‹Overview\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\nâ€‹Concepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\n\\nOnce weâ€™ve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or youâ€™re comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\nâ€‹Preview\\nIn this guide weâ€™ll build an app that answers questions about the websiteâ€™s content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)\\n\\n# Construct a tool for retrieving context\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\\n  Args:\\n    query: task decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done by...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nTask decomposition refers to...\\nCheck out the LangSmith trace.\\nâ€‹Setup\\nâ€‹Installation\\nThis tutorial requires these langchain dependencies:\\npipuvcondaCopypip install langchain langchain-text-splitters langchain-community bs4\\n\\nFor more details, see our Installation guide.\\nâ€‹LangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\nâ€‹Components\\nWe will need to select three components from LangChainâ€™s suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS BedrockğŸ‘‰ Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx Fake IsaacusCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n\\nSelect a vector store:\\n In-memory AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopypip install -U \"langchain-core\"\\nCopyfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\n\\nâ€‹1. Indexing\\nThis section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if youâ€™re comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\\nIndexing commonly works as follows:\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and wonâ€™t fit in a modelâ€™s finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\nâ€‹Loading documents\\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\\nIn this case weâ€™ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class â€œpost-contentâ€�, â€œpost-titleâ€�, or â€œpost-headerâ€� are relevant, so weâ€™ll remove all others.\\nCopyimport bs4\\nfrom langchain_community.document_loaders import WebBaseLoader\\n\\n# Only keep post title, headers, and content from the full HTML.\\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs={\"parse_only\": bs4_strainer},\\n)\\ndocs = loader.load()\\n\\nassert len(docs) == 1\\nprint(f\"Total characters: {len(docs[0].page_content)}\")\\n\\nCopyTotal characters: 43131\\n\\nCopyprint(docs[0].page_content[:500])\\n\\nCopy      LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn\\n\\nGo deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.\\n\\nâ€‹Splitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\\nTo handle this weâ€™ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\nCopyfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,  # chunk size (characters)\\n    chunk_overlap=200,  # chunk overlap (characters)\\n    add_start_index=True,  # track index in original document\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\\n\\nCopySplit blog post into 66 sub-documents.\\n\\nGo deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.\\n\\nâ€‹Storing documents\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\\nCopydocument_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nCopy[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\n\\nGo deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\\n\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\\n\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\nâ€‹2. Retrieval and Generation\\nRAG applications commonly work as follows:\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nNow letâ€™s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\\nWe will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\nâ€‹RAG agents\\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\\nCopyfrom langchain.tools import tool\\n\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\\nRetrieval tools are not limited to a single string query argument, as in the above example. You can\\nforce the LLM to specify additional search parameters by adding argumentsâ€” for example, a category:Copyfrom typing import Literal\\n\\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\\n\\nGiven our tool, we can construct the agent:\\nCopyfrom langchain.agents import create_agent\\n\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\n\\nLetâ€™s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nCopyquery = (\\n    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"\\n    \"Once you get the answer, look up common extensions of that method.\"\\n)\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    event[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\\n  Args:\\n    query: standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\\n  Args:\\n    query: common extensions of the standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\\n\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directlyâ€” for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraphâ€™s Agentic RAG tutorial for more advanced formulations.\\nâ€‹RAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\\nâœ… Benefitsâš\\xa0ï¸� DrawbacksSearch only when needed â€“ The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.Two inference calls â€“ When a search is performed, it requires one call to generate the query and another to produce the final response.Contextual search queries â€“ By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.Reduced control â€“ The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.Multiple searches allowed â€“ The LLM can execute several searches in support of a single user query.\\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\\nIn this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n@dynamic_prompt\\ndef prompt_with_context(request: ModelRequest) -> str:\\n    \"\"\"Inject context into state messages.\"\"\"\\n    last_query = request.state[\"messages\"][-1].text\\n    retrieved_docs = vector_store.similarity_search(last_query)\\n\\n    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n    system_message = (\\n        \"You are a helpful assistant. Use the following context in your response:\"\\n        f\"\\\\n\\\\n{docs_content}\"\\n    )\\n\\n    return system_message\\n\\n\\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\\n\\nLetâ€™s try this out:\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\n\\nTask decomposition is...\\n\\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\\nReturning source documentsThe above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\\nAdding a key to the state to store the retrieved documents\\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\\nCopyfrom typing import Any\\nfrom langchain_core.documents import Document\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\\n\\n\\nclass State(AgentState):\\n    context: list[Document]\\n\\n\\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\\n    state_schema = State\\n\\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\\n        last_message = state[\"messages\"][-1]\\n        retrieved_docs = vector_store.similarity_search(last_message.text)\\n\\n        docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n        augmented_message_content = (\\n            f\"{last_message.text}\\\\n\\\\n\"\\n            \"Use the following context to answer the query:\\\\n\"\\n            f\"{docs_content}\"\\n        )\\n        return {\\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\\n            \"context\": retrieved_docs,\\n        }\\n\\n\\nagent = create_agent(\\n    model,\\n    tools=[],\\n    middleware=[RetrieveDocumentsMiddleware()],\\n)\\n\\nâ€‹Next steps\\nNow that weâ€™ve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployments\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document=loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content=\"Build a RAG agent with LangChain - Docs by LangChainSkip to main contentğŸš€ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentSupervisor agentVoice agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy pageâ€‹Overview\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='A RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\nâ€‹Concepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Once weâ€™ve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or youâ€™re comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\nâ€‹Preview\\nIn this guide weâ€™ll build an app that answers questions about the websiteâ€™s content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Expand for full code snippetCopyimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)\\n\\n# Construct a tool for retrieving context\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='tools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\\n  Args:\\n    query: task decomposition\\n================================= Tool Message =================================\\nName: retrieve_context'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content=\"Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\\nContent: Task decomposition can be done by...\\n\\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nTask decomposition refers to...\\nCheck out the LangSmith trace.\\nâ€‹Setup\\nâ€‹Installation\\nThis tutorial requires these langchain dependencies:\\npipuvcondaCopypip install langchain langchain-text-splitters langchain-community bs4\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='For more details, see our Installation guide.\\nâ€‹LangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Or, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\nâ€‹Components\\nWe will need to select three components from LangChainâ€™s suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS BedrockğŸ‘‰ Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx Fake IsaacusCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='if not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n\\nSelect a vector store:\\n In-memory AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopypip install -U \"langchain-core\"\\nCopyfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\n\\nâ€‹1. Indexing\\nThis section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if youâ€™re comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\\nIndexing commonly works as follows:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Load: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and wonâ€™t fit in a modelâ€™s finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='â€‹Loading documents\\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\\nIn this case weâ€™ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class â€œpost-contentâ€�, â€œpost-titleâ€�, or â€œpost-headerâ€� are relevant, so weâ€™ll remove all others.\\nCopyimport bs4\\nfrom langchain_community.document_loaders import WebBaseLoader\\n\\n# Only keep post title, headers, and content from the full HTML.\\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs={\"parse_only\": bs4_strainer},\\n)\\ndocs = loader.load()'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='assert len(docs) == 1\\nprint(f\"Total characters: {len(docs[0].page_content)}\")\\n\\nCopyTotal characters: 43131\\n\\nCopyprint(docs[0].page_content[:500])\\n\\nCopy      LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn\\n\\nGo deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Go deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.\\n\\nâ€‹Splitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\\nTo handle this weâ€™ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\nCopyfrom langchain_text_splitters import RecursiveCharacterTextSplitter'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='text_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,  # chunk size (characters)\\n    chunk_overlap=200,  # chunk overlap (characters)\\n    add_start_index=True,  # track index in original document\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\\n\\nCopySplit blog post into 66 sub-documents.\\n\\nGo deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content=\"Go deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.\\n\\nâ€‹Storing documents\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\\nCopydocument_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nCopy['07c18af6-ad58-479a-bfb1-d508033f9c64', '9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6']\\n\\nGo deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Go deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\\n\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\\n\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\nâ€‹2. Retrieval and Generation\\nRAG applications commonly work as follows:\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Now letâ€™s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\\nWe will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\nâ€‹RAG agents\\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\\nCopyfrom langchain.tools import tool'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\\nRetrieval tools are not limited to a single string query argument, as in the above example. You can\\nforce the LLM to specify additional search parameters by adding argumentsâ€” for example, a category:Copyfrom typing import Literal\\n\\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='def retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\\n\\nGiven our tool, we can construct the agent:\\nCopyfrom langchain.agents import create_agent\\n\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\n\\nLetâ€™s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nCopyquery = (\\n    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"\\n    \"Once you get the answer, look up common extensions of that method.\"\\n)\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    event[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message ================================='),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content=\"Copy================================ Human Message =================================\\n\\nWhat is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\\n  Args:\\n    query: standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\\nContent: Task decomposition can be done...\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content=\"Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\\nContent: Task decomposition can be done...\\n\\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\\n  Args:\\n    query: common extensions of the standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\\nContent: Task decomposition can be done...\\n\\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content=\"Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\\n\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directlyâ€” for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraphâ€™s Agentic RAG tutorial for more advanced formulations.\\nâ€‹RAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='âœ… Benefitsâš\\xa0ï¸� DrawbacksSearch only when needed â€“ The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.Two inference calls â€“ When a search is performed, it requires one call to generate the query and another to produce the final response.Contextual search queries â€“ By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.Reduced control â€“ The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.Multiple searches allowed â€“ The LLM can execute several searches in support of a single user query.\\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='In this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyfrom langchain.agents.middleware import dynamic_prompt, ModelRequest'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='@dynamic_prompt\\ndef prompt_with_context(request: ModelRequest) -> str:\\n    \"\"\"Inject context into state messages.\"\"\"\\n    last_query = request.state[\"messages\"][-1].text\\n    retrieved_docs = vector_store.similarity_search(last_query)\\n\\n    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n    system_message = (\\n        \"You are a helpful assistant. Use the following context in your response:\"\\n        f\"\\\\n\\\\n{docs_content}\"\\n    )\\n\\n    return system_message\\n\\n\\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\\n\\nLetâ€™s try this out:\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\n\\nTask decomposition is...'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Task decomposition is...\\n\\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\\nReturning source documentsThe above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\\nAdding a key to the state to store the retrieved documents\\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\\nCopyfrom typing import Any\\nfrom langchain_core.documents import Document\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\\n\\n\\nclass State(AgentState):\\n    context: list[Document]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='class State(AgentState):\\n    context: list[Document]\\n\\n\\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\\n    state_schema = State\\n\\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\\n        last_message = state[\"messages\"][-1]\\n        retrieved_docs = vector_store.similarity_search(last_message.text)\\n\\n        docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n        augmented_message_content = (\\n            f\"{last_message.text}\\\\n\\\\n\"\\n            \"Use the following context to answer the query:\\\\n\"\\n            f\"{docs_content}\"\\n        )\\n        return {\\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\\n            \"context\": retrieved_docs,\\n        }\\n\\n\\nagent = create_agent(\\n    model,\\n    tools=[],\\n    middleware=[RetrieveDocumentsMiddleware()],\\n)\\n\\nâ€‹Next steps\\nNow that weâ€™ve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='â€‹Next steps\\nNow that weâ€™ve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployments\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(document)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.1-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Downloading faiss_cpu-1.13.1-cp311-cp311-win_amd64.whl (18.8 MB)\n",
      "   ---------------------------------------- 0.0/18.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/18.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/18.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/18.8 MB 217.9 kB/s eta 0:01:27\n",
      "   ---------------------------------------- 0.1/18.8 MB 363.1 kB/s eta 0:00:52\n",
      "    --------------------------------------- 0.3/18.8 MB 1.3 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 0.8/18.8 MB 3.3 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 2.8/18.8 MB 9.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 5.8/18.8 MB 17.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 10.2/18.8 MB 27.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 12.7/18.8 MB 72.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 15.0/18.8 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 17.1/18.8 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.8/18.8 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.8/18.8 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.8/18.8 MB 36.4 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.13.1\n"
     ]
    }
   ],
   "source": [
    "! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x17c3a615790>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore=FAISS.from_documents(documents,embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"This is a relatively simple LLM application \"\n",
    "\n",
    "result=vectorstore.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-classic in c:\\users\\abcde\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-classic) (1.1.1)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-classic) (1.0.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.17 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-classic) (0.4.56)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-classic) (2.10.3)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-classic) (6.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-classic) (2.32.5)\n",
      "Requirement already satisfied: sqlalchemy<3.0.0,>=1.4.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-classic) (2.0.25)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-classic) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-classic) (25.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-classic) (8.2.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-classic) (4.12.2)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-classic) (0.12.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.17->langchain-classic) (0.26.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.17->langchain-classic) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.17->langchain-classic) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.17->langchain-classic) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic) (2.27.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->langchain-classic) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->langchain-classic) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->langchain-classic) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->langchain-classic) (2025.4.26)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from sqlalchemy<3.0.0,>=1.4.0->langchain-classic) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain-classic) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain-classic) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain-classic) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain-classic) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abcde\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-classic) (2.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain-classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"What are everyone's favorite colors:\\n\\n{context}\"), additional_kwargs={})])\n",
       "| ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x0000017C76BC1D90>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000017C3A644C90>, root_client=<openai.OpenAI object at 0x0000017C76BCD4D0>, root_async_client=<openai.AsyncOpenAI object at 0x0000017C76BDEE90>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install -U langchain langchain-community\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"What are everyone's favorite colors:\\n\\n{context}\")]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the information provided:\\n\\n- Jesse's favorite color is red, and he does not love yellow.\\n- Jamal's favorite color is orange, but he also loves green, although not as much as orange.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\n",
    "    Document(page_content=\"Jesse loves red but not yellow\"),\n",
    "    Document(page_content = \"Jamal loves green but not as much as he loves orange\")\n",
    "]\n",
    "\n",
    "chain.invoke({\"context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"What are everyone's favorite colors:\\n\\n{context}\"), additional_kwargs={})])\n",
       "| ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x0000017C76BC1D90>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000017C3A644C90>, root_client=<openai.OpenAI object at 0x0000017C76BCD4D0>, root_async_client=<openai.AsyncOpenAI object at 0x0000017C76BDEE90>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f8be1bc4-b6f2-4e05-b4e8-2d559333ebe2', metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Or, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\nâ€‹Components\\nWe will need to select three components from LangChainâ€™s suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS BedrockğŸ‘‰ Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx Fake IsaacusCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings'),\n",
       " Document(id='9c372d76-725d-4b20-95f7-d321f5058229', metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='In this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyfrom langchain.agents.middleware import dynamic_prompt, ModelRequest'),\n",
       " Document(id='ba14d8b3-3fc9-4a2c-b413-af75bd0d4882', metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:'),\n",
       " Document(id='4f435709-290b-4439-baf8-133df8d9fd46', metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='â€‹Next steps\\nNow that weâ€™ve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployments\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"Note that ChatModels receive message objects as inpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI language model, I don't have personal preferences or emotions, but I can share that people often have a variety of favorite colors. Common favorites include blue, green, red, purple, and yellow. A person's favorite color can be influenced by cultural factors, personal experiences, and individual tastes. If you'd like to share your favorite color, I'd be happy to hear it!\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain.invoke({\n",
    "    \"input\":\"Note that ChatModels receive message objects as input\",\n",
    "    \"context\":[Document(page_content=\"Note that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\\nmodel.invoke('Hello')model.invoke([{'role': 'user', 'content': 'Hello'}])model.invoke([HumanMessage('Hello')]) StreamingBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\\nfor token in model.stream(messages):    print(token.content, end='|')|C|iao|!|| You can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000017C3A615790>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"What are everyone's favorite colors:\\n\\n{context}\"), additional_kwargs={})])\n",
       "            | ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x0000017C76BC1D90>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000017C3A644C90>, root_client=<openai.OpenAI object at 0x0000017C76BCD4D0>, root_async_client=<openai.AsyncOpenAI object at 0x0000017C76BDEE90>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=vectorstore.as_retriever()\n",
    "\n",
    "from langchain_classic.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=retrieval_chain.invoke({\"input\":\"Note that ChatModels receive message objects as input\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Building a Q&A chatbot using Retrieval Augmented Generation (RAG) involves integrating language models and embedding models to interact with and retrieve information from unstructured data sources. Here’s a high-level overview of how you can achieve this with LangChain:\\n\\n### Components Required:\\n\\n1. **Chat Model Selection:**\\n   - You need a chat model to handle natural language interaction. Models like OpenAI\\'s GPT series are popular choices. You can initialize your chat model with LangChain using:\\n     ```python\\n     from langchain.chat_models import init_chat_model\\n     model = init_chat_model(\"gpt-4.1\")\\n     ```\\n\\n2. **Embeddings Model:**\\n   - Embeddings are critical for turning text into a numerical format that a model can understand and process. Models from OpenAI, Cohere, or HuggingFace could be used for embeddings.\\n     ```python\\n     from langchain_openai import OpenAIEmbeddings\\n     ```\\n\\n### Steps to Build RAG Application:\\n\\n1. **Environment Setup:**\\n   - Ensure your environment variables such as API keys are set securely. This can be done using Python’s `os.environ` or `getpass` to prompt the user for input.\\n\\n2. **Configure Model Integration:**\\n   - Install necessary dependencies using pip and configure your models by setting API keys and following any setup instructions provided by the model\\'s SDK or library.\\n\\n3. **Implement Retrieval Augmented Generation:**\\n   - Use LangChain’s `create_agent` capability to seamlessly combine retrieval steps and generative steps. This allows your application to retrieve relevant information before generating a response.\\n   - Create custom prompts that incorporate retrieved data into your conversation with the model, allowing dynamic question answering based upon retrieved context.\\n\\n### Advanced Features to Consider:\\n\\n- **Streaming Responses:** Implement token streaming for more responsive user interactions.\\n- **Memory Management:**\\n  - **Conversational Memory:** Track multi-turn interactions to maintain context.\\n  - **Long-Term Memory:** Store information across multiple sessions or interactions.\\n  \\n- **Structured Responses:** Customize the format of responses to suit your application’s needs.\\n  \\n- **Deployment:** Utilize LangChain\\'s deployment tools for scaling and maintaining your application.\\n\\n### Feedback and Iteration:\\n\\n- Continuously test and improve your application. Use feedback mechanisms to adjust your prompts and retrieval strategies to fine-tune the chatbot’s performance and relevance.\\n  \\n- Engage with community resources, forums, and documentation for support and updates.\\n\\nThis foundational setup provides a scalable path to building sophisticated, dynamic Q&A applications harnessing the power of modern machine learning models with LangChain\\'s ecosystem.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
